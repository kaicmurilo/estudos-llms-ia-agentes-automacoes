# üìÑ LlamaIndex com Ollama (Local LLM)

Este projeto utiliza o [LlamaIndex](https://llamaindex.ai) com o modelo local via [Ollama](https://ollama.com/) para realizar **indexa√ß√£o inteligente de documentos locais** e permitir **consultas em linguagem natural**, com suporte a arquivos `.pdf`.

---

## ‚úÖ Funcionalidades

- Suporte a documentos `.pdf` (e expans√≠vel para `.txt`, `.docx`, etc.)
- Indexa√ß√£o persistente com verifica√ß√£o de arquivos novos
- LLM local usando Ollama (sem depender da OpenAI)
- Consulta com respostas estruturadas em JSON
- Detec√ß√£o autom√°tica de documentos alterados ou adicionados

---

## üöÄ Como rodar

### 1. Instale o [Ollama](https://ollama.com/)
> Ex: `ollama run llama3`

### 2. Clone o reposit√≥rio e crie o ambiente

```bash
git clone https://github.com/seu-usuario/seu-projeto.git
cd seu-projeto
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

3. Crie a pasta data/ e adicione seus arquivos

```bash
mkdir data
# coloque seus PDFs ou outros arquivos aqui
```

4. Execute o app

```bash
python app.py
```

üß† Exemplo de Consulta

```bash
{
  "query": "Quem toma caf√©? Qual a quantidade de caf√© ele toma?",
  "response": "Kaic toma de 3 a 4 copos por dia..."
}
```

üì¶ Requisitos

Python 3.10+

Ollama (com modelo baixado, ex: llama3)

Linux/MacOS (ou WSL no Windows)


‚úçÔ∏è Expans√µes futuras

Suporte a .docx, .csv, .md

UI com Streamlit

Cache de respostas para acelerar consultas repetidas

